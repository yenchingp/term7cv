{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rcmalli/keras-squeezenet.git\n",
      "  Cloning https://github.com/rcmalli/keras-squeezenet.git to /private/var/folders/gm/h5nn__xj4n997658bxh_9t800000gn/T/pip-req-build-7fsluy1v\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/rcmalli/keras-squeezenet.git /private/var/folders/gm/h5nn__xj4n997658bxh_9t800000gn/T/pip-req-build-7fsluy1v\n",
      "  Resolved https://github.com/rcmalli/keras-squeezenet.git to commit 4fb9cb7510ea0315303090edbc1bd97c2916af81\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (1.25.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (1.11.3)\n",
      "Requirement already satisfied: h5py in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (3.10.0)\n",
      "Requirement already satisfied: tensorflow in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (2.14.0)\n",
      "Requirement already satisfied: keras in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (2.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (1.16.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.11/site-packages (from keras-squeezenet==0.4) (6.0.1)\n",
      "Requirement already satisfied: tensorflow-macos==2.14.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow->keras-squeezenet==0.4) (2.14.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (3.3.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (4.25.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (68.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (1.59.2)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.14.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.14.0)\n",
      "Collecting keras (from keras-squeezenet==0.4)\n",
      "  Obtaining dependency information for keras from https://files.pythonhosted.org/packages/fe/58/34d4d8f1aa11120c2d36d7ad27d0526164b1a8ae45990a2fede31d0e59bf/keras-2.14.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/homebrew/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/homebrew/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/homebrew/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/homebrew/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/homebrew/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/homebrew/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/homebrew/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow-macos==2.14.0->tensorflow->keras-squeezenet==0.4) (3.2.2)\n",
      "Using cached keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: Keras 2.1.1\n",
      "    Uninstalling Keras-2.1.1:\n",
      "      Successfully uninstalled Keras-2.1.1\n",
      "Successfully installed keras-2.14.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.14.0rc1 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.15.0rc0, 2.15.0rc1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.14.0rc1\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: q in /opt/homebrew/lib/python3.11/site-packages (2.7)\n",
      "Collecting keras==2.1.1\n",
      "  Using cached Keras-2.1.1-py2.py3-none-any.whl (302 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/homebrew/lib/python3.11/site-packages (from keras==2.1.1) (1.25.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from keras==2.1.1) (1.11.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/homebrew/lib/python3.11/site-packages (from keras==2.1.1) (1.16.0)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.11/site-packages (from keras==2.1.1) (6.0.1)\n",
      "Installing collected packages: keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.14.0\n",
      "    Uninstalling keras-2.14.0:\n",
      "      Successfully uninstalled keras-2.14.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-macos 2.14.0 requires keras<2.15,>=2.14.0, but you have keras 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed keras-2.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/rcmalli/keras-squeezenet.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Feature Extraction Models\n",
    "- mobilenet\n",
    "- resnet\n",
    "- densenet\n",
    "- efficientnet\n",
    "- vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Each model's specific import and preprocess_input\n",
    "from tensorflow.keras.applications import mobilenet, resnet, densenet, efficientnet, vgg19\n",
    "# from keras_squeezenet import SqueezeNet\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_and_preprocess_image(img_path, model_name):\n",
    "    target_size = (224, 224)  # default target size for most models\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img = image.img_to_array(img)\n",
    "    \n",
    "    # Preprocess input based on model\n",
    "    if model_name == 'mobilenet':\n",
    "        img = mobilenet.preprocess_input(img)\n",
    "    elif model_name == 'resnet':\n",
    "        img = resnet.preprocess_input(img)\n",
    "    elif model_name == 'densenet':\n",
    "        img = densenet.preprocess_input(img)\n",
    "    elif model_name == 'efficientnet':\n",
    "        img = efficientnet.preprocess_input(img)\n",
    "    elif model_name == 'vgg19':\n",
    "        img = vgg19.preprocess_input(img)\n",
    "    \n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Function to create a model for feature extraction\n",
    "def create_feature_model(model_name):\n",
    "    if model_name == 'mobilenet':\n",
    "        base_model = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
    "    elif model_name == 'resnet':\n",
    "        base_model = resnet.ResNet50(weights='imagenet', include_top=False)\n",
    "    elif model_name == 'densenet':\n",
    "        base_model = densenet.DenseNet121(weights='imagenet', include_top=False)\n",
    "    elif model_name == 'efficientnet':\n",
    "        base_model = efficientnet.EfficientNetB0(weights='imagenet', include_top=False)\n",
    "    elif model_name == 'vgg19':\n",
    "        base_model = vgg19.VGG19(weights='imagenet', include_top=False)\n",
    "    # Pooling\n",
    "    # x = base_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # feature_model = Model(inputs=base_model.input, outputs=x)\n",
    "    feature_model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "    return feature_model\n",
    "\n",
    "# Feature extraction for an image\n",
    "def extract_features(img_path, feature_model, model_name):\n",
    "    img = load_and_preprocess_image(img_path, model_name)\n",
    "    features = feature_model.predict(img)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "1/1 [==============================] - 0s 203ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "mobilenet took 0.49 seconds\n",
      "1/1 [==============================] - 0s 364ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "resnet took 0.89 seconds\n",
      "1/1 [==============================] - 1s 772ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "densenet took 1.35 seconds\n",
      "1/1 [==============================] - 0s 493ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "efficientnet took 0.86 seconds\n",
      "1/1 [==============================] - 0s 130ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "vgg19 took 1.00 seconds\n",
      "mobilenet: 0.49 seconds\n",
      "resnet: 0.89 seconds\n",
      "densenet: 1.35 seconds\n",
      "efficientnet: 0.86 seconds\n",
      "vgg19: 1.00 seconds\n",
      "The fastest model is mobilenet with 0.49 seconds.\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"/Users/kaavi/Documents/GitHub/term7cv/dataset/objects/train_8\"  \n",
    "test_image_paths = [os.path.join(img_dir, f) for f in os.listdir(img_dir)[:10]]  \n",
    "\n",
    "model_names = ['mobilenet', 'resnet', 'densenet', 'efficientnet', 'vgg19']\n",
    "times = {}\n",
    "\n",
    "for model_name in model_names:\n",
    "    feature_model = create_feature_model(model_name)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for img_path in test_image_paths:\n",
    "        _ = extract_features(img_path, feature_model, model_name)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    times[model_name] = duration\n",
    "    print(f\"{model_name} took {duration:.2f} seconds\")\n",
    "\n",
    "# Compare speeds\n",
    "for model_name, duration in times.items():\n",
    "    print(f\"{model_name}: {duration:.2f} seconds\")\n",
    "\n",
    "# Find fastest model\n",
    "fastest_model = min(times, key=times.get)\n",
    "print(f\"The fastest model is {fastest_model} with {times[fastest_model]:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import mobilenet_v2\n",
    "model = mobilenet_v2.MobileNetV2(weights='imagenet', include_top=False, pooling='avg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mobilenet_features(image_path, model):\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=(224, 224))  # size 224x224\n",
    "    img = image.img_to_array(img)\n",
    "    img = mobilenet_v2.preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    # Extract features\n",
    "    features = model.predict(img)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 300ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "img_dir = \"/Users/kaavi/Documents/GitHub/term7cv/dataset/objects/train_8\"\n",
    "\n",
    "features_dict = {}\n",
    "\n",
    "# Assuming that os.listdir(img_dir) returns a list of image filenames in img_dir\n",
    "for img_name in os.listdir(img_dir):\n",
    "    image_path = os.path.join(img_dir, img_name)\n",
    "    if image_path.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check to ensure only images are processed\n",
    "        features_dict[img_name] = extract_mobilenet_features(image_path, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten features (if not already flattened by global average pooling -> pooling='avg')\n",
    "feature_vectors = np.array([features_dict[img_name].flatten() for img_name in features_dict.keys()])\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "feature_vectors_normalized = scaler.fit_transform(feature_vectors)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=0.9)  # Keep 90% of the variance\n",
    "principal_components = pca.fit_transform(feature_vectors_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 73)\n"
     ]
    }
   ],
   "source": [
    "print(principal_components.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Dimensionality Reduction\n",
    "\n",
    "- PCA is designed for variance maximization and dimensionality reduction, so it naturally fits the goal of retaining a high percentage of the variance.\n",
    "- t-SNE does not retain variance in the same way PCA does. It's more suitable for visualization in two or three dimensions and does not provide an explained variance ratio.\n",
    "- UMAP is generally used for visualization and dimensionality reduction while preserving the data's local and global structure. It's a non-linear technique, which means it may reveal structures hidden to PCA but does not directly provide an explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature size: 146 1280\n",
      "PCA reduced feature size: 146 73\n",
      "t-SNE reduced feature size (not directly comparable): 146 2\n",
      "UMAP reduced feature size: 146 3\n",
      "PCA explained variance ratio: [0.10797926 0.07957169 0.06185418 0.05162408 0.04534406 0.03569248\n",
      " 0.02883516 0.02737654 0.02265139 0.02056257 0.01902802 0.01742499\n",
      " 0.01671618 0.01590149 0.01477082 0.01341038 0.0127011  0.0125001\n",
      " 0.01139044 0.01127039 0.01100684 0.01028406 0.00967457 0.00943174\n",
      " 0.00905472 0.00834843 0.00801593 0.00795086 0.00777307 0.00739645\n",
      " 0.00709203 0.00689129 0.00675144 0.00662942 0.00625688 0.00615374\n",
      " 0.0059451  0.0058987  0.00576914 0.00549073 0.00548384 0.00539433\n",
      " 0.00519351 0.00505459 0.00496324 0.0048778  0.00462645 0.00449609\n",
      " 0.00431962 0.00426983 0.00411938 0.00405598 0.00392863 0.00383698\n",
      " 0.00379545 0.003708   0.00362123 0.00357388 0.0034829  0.00332994\n",
      " 0.00321283 0.00318638 0.00311156 0.00303781 0.00300412 0.00295493\n",
      " 0.00284284 0.0027912  0.0027231  0.00267491 0.00260298 0.00256463\n",
      " 0.00252795]\n",
      "PCA cumulative explained variance: 0.90178734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming feature_vectors_normalized from the previous code block after StandardScaler\n",
    "\n",
    "# PCA Dimensionality Reduction\n",
    "pca = PCA(0.9)  # Keep 90% of the variance\n",
    "pca_result = pca.fit_transform(feature_vectors_normalized)\n",
    "\n",
    "# t-SNE Dimensionality Reduction - It does not have variance ratio\n",
    "# t-SNE is not typically used for variance retention comparison, and it's a stochastic method.\n",
    "# Therefore, it is more about visualization than retaining variance.\n",
    "# We perform t-SNE on PCA output to speed up the process, this is optional but recommended for large datasets\n",
    "tsne_result = TSNE(n_components=2, learning_rate='auto', init='pca').fit_transform(pca_result)\n",
    "\n",
    "# UMAP Dimensionality Reduction - It also doesn't work with explained variance\n",
    "# Instead, it works with neighbor graphs to preserve local and global structure.\n",
    "umap_result = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=3).fit_transform(feature_vectors_normalized)\n",
    "\n",
    "# Now, let's compare the shapes\n",
    "print('Original feature size:', feature_vectors_normalized.shape[0], feature_vectors_normalized.shape[1])\n",
    "print('PCA reduced feature size:', pca_result.shape[0], pca_result.shape[1])\n",
    "print('t-SNE reduced feature size (not directly comparable):', tsne_result.shape[0], tsne_result.shape[1])\n",
    "print('UMAP reduced feature size:', umap_result.shape[0], umap_result.shape[1])\n",
    "\n",
    "# Print the amount of variance that each component contributes\n",
    "print('PCA explained variance ratio:', pca.explained_variance_ratio_)\n",
    "\n",
    "# If you want to also check the actual size of the reduced data you can compare the sum of the explained_variance_ratio_\n",
    "print('PCA cumulative explained variance:', np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/umap/umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "\n",
    "feature_list = [features for features in features_dict.values()]\n",
    "feature_matrix = np.array(feature_list).reshape(len(feature_list), -1) # Reshape to (n_samples, n_features) if necessary\n",
    "\n",
    "# Apply UMAP\n",
    "reducer = umap.UMAP(n_neighbors=15, n_components=2, metric='euclidean', random_state=42)\n",
    "embedding = reducer.fit_transform(feature_matrix)\n",
    "\n",
    "reduced_features = {name: emb for name, emb in zip(features_dict.keys(), embedding)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Clustering Algorithms\n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise): It can find arbitrarily shaped clusters and can have a notion of noise, which are points that don't fit well into any cluster.\n",
    "- HDBSCAN (Hierarchical DBSCAN): An extension of DBSCAN that converts it into a hierarchical clustering algorithm and enables it to find clusters of varying densities, which DBSCAN cannot do.\n",
    "- Mean Shift: It doesn't require the specification of the number of clusters, as it automatically finds clusters based on data density. However, it does require bandwidth parameter selection, which indirectly influences the number of clusters.\n",
    "- Affinity Propagation: It uses message passing between data points to create clusters based on their similarity. It does not require the number of clusters to be specified and can yield a varying number of clusters based on the input data and preferences parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters for DBSCAN: 8\n",
      "Estimated number of clusters for HDBSCAN: 6\n",
      "Estimated number of clusters for Mean Shift: 3\n",
      "Estimated number of clusters for Affinity Propagation: 6\n",
      "Silhouette Coefficient for DBSCAN: 0.684\n",
      "Silhouette Coefficient for HDBSCAN: 0.652\n",
      "Silhouette Coefficient for Mean Shift: 0.619\n",
      "Silhouette Coefficient for Affinity Propagation: 0.614\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN, MeanShift, estimate_bandwidth, AffinityPropagation\n",
    "import hdbscan\n",
    "from sklearn import metrics\n",
    "\n",
    "# 'reduced_features' to list of feature coordinates for clustering\n",
    "X = np.array(list(reduced_features.values()))\n",
    "\n",
    "# DBSCAN\n",
    "db = DBSCAN(eps=0.5, min_samples=5).fit(X)\n",
    "db_labels = db.labels_\n",
    "\n",
    "# HDBSCAN\n",
    "hdb = hdbscan.HDBSCAN(min_cluster_size=5).fit(X)\n",
    "hdb_labels = hdb.labels_\n",
    "\n",
    "# Mean Shift\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2)\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True).fit(X)\n",
    "ms_labels = ms.labels_\n",
    "\n",
    "# Affinity Propagation\n",
    "af = AffinityPropagation().fit(X)\n",
    "af_labels = af.labels_\n",
    "\n",
    "# Calculating the number of clusters for each algorithm\n",
    "n_clusters_db = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "n_clusters_hdb = len(set(hdb_labels)) - (1 if -1 in hdb_labels else 0)\n",
    "n_clusters_ms = len(set(ms_labels))\n",
    "n_clusters_af = len(set(af_labels))\n",
    "\n",
    "print(f\"Estimated number of clusters for DBSCAN: {n_clusters_db}\")\n",
    "print(f\"Estimated number of clusters for HDBSCAN: {n_clusters_hdb}\")\n",
    "print(f\"Estimated number of clusters for Mean Shift: {n_clusters_ms}\")\n",
    "print(f\"Estimated number of clusters for Affinity Propagation: {n_clusters_af}\")\n",
    "\n",
    "# measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation)\n",
    "silhouette_db = metrics.silhouette_score(X, db_labels) if n_clusters_db > 1 else -1\n",
    "silhouette_hdb = metrics.silhouette_score(X, hdb_labels) if n_clusters_hdb > 1 else -1\n",
    "silhouette_ms = metrics.silhouette_score(X, ms_labels) if n_clusters_ms > 1 else -1\n",
    "silhouette_af = metrics.silhouette_score(X, af_labels) if n_clusters_af > 1 else -1\n",
    "\n",
    "print(f\"Silhouette Coefficient for DBSCAN: {silhouette_db:.3f}\")\n",
    "print(f\"Silhouette Coefficient for HDBSCAN: {silhouette_hdb:.3f}\")\n",
    "print(f\"Silhouette Coefficient for Mean Shift: {silhouette_ms:.3f}\")\n",
    "print(f\"Silhouette Coefficient for Affinity Propagation: {silhouette_af:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inventory count for each object type (cluster):\n",
      "Object type (Cluster 0) instances: 20\n",
      "Object type (Cluster 1) instances: 59\n",
      "Object type (Cluster 2) instances: 5\n",
      "Object type (Cluster 7) instances: 23\n",
      "Object type (Cluster 3) instances: 6\n",
      "Object type (Cluster 4) instances: 17\n",
      "Object type (Cluster 5) instances: 9\n",
      "Object type (Cluster 6) instances: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "cluster_dir_base = os.path.join(img_dir, 'clusters')\n",
    "os.makedirs(cluster_dir_base, exist_ok=True)\n",
    "\n",
    "inventory_count = {}\n",
    "\n",
    "for img_name, cluster_label in zip(features_dict.keys(), db_labels):\n",
    "    if cluster_label == -1:\n",
    "        # -1 for noise points\n",
    "        cluster_path = os.path.join(cluster_dir_base, 'noise')\n",
    "    else:\n",
    "        cluster_path = os.path.join(cluster_dir_base, f'cluster_{cluster_label}')\n",
    "    \n",
    "    os.makedirs(cluster_path, exist_ok=True)\n",
    "    \n",
    "    source = os.path.join(img_dir, img_name)\n",
    "    destination = os.path.join(cluster_path, img_name)\n",
    "    shutil.copy(source, destination)\n",
    "    \n",
    "    inventory_count[cluster_label] = inventory_count.get(cluster_label, 0) + 1\n",
    "\n",
    "# Inventory count for each cluster\n",
    "print(\"Inventory count for each object type (cluster):\")\n",
    "for cluster_label, count in inventory_count.items():\n",
    "    if cluster_label == -1:\n",
    "        print(f\"Noise (unclustered) instances: {count}\")\n",
    "    else:\n",
    "        print(f\"Object type (Cluster {cluster_label}) instances: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
