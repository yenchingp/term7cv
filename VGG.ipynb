{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term 7 project: Inventory management\n",
    "### Link to Original Github Repo: https://github.com/eg4000/SKU110K_CVPR19\n",
    "### SKU-110K dataset can be found here: https://www.kaggle.com/datasets/thedatasith/sku110k-annotations/data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.experimental.list_physical_devices('GPU'))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dotenv\n",
    "\n",
    "# create your own .env file and add in your directory in the format\n",
    "# SKU_DATASET_DIR = r\"your_directory\"\n",
    "dotenv.load_dotenv()\n",
    "sku_dataset_dir =  os.getenv(\"SKU_DATASET_DIR\")\n",
    "annotations_dir = Path(sku_dataset_dir) / 'annotations'\n",
    "images_path = Path(sku_dataset_dir) / 'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete corrupted images\n",
    "- List: https://github.com/eg4000/SKU110K_CVPR19/issues/99#issuecomment-988886374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPTED_IMAGES = {\n",
    "    \"train\": (\n",
    "              \"train_1239.jpg\",\n",
    "              \"train_2376.jpg\",\n",
    "              \"train_2903.jpg\",\n",
    "              \"train_2986.jpg\",\n",
    "              \"train_305.jpg\",\n",
    "              \"train_3240.jpg\",\n",
    "              \"train_340.jpg\",\n",
    "              \"train_3556.jpg\",\n",
    "              \"train_3560.jpg\",\n",
    "              \"train_3832.jpg\",\n",
    "              \"train_38.jpg\",\n",
    "              \"train_4222.jpg\",\n",
    "              \"train_5007.jpg\",\n",
    "              \"train_5137.jpg\",\n",
    "              \"train_5143.jpg\",\n",
    "              \"train_5762.jpg\",\n",
    "              \"train_5822.jpg\",\n",
    "              \"train_6052.jpg\",\n",
    "              \"train_6090.jpg\",\n",
    "              \"train_6138.jpg\",\n",
    "              \"train_6409.jpg\",\n",
    "              \"train_6722.jpg\",\n",
    "              \"train_6788.jpg\",\n",
    "              \"train_737.jpg\",\n",
    "              \"train_7576.jpg\",\n",
    "              \"train_7622.jpg\",\n",
    "              \"train_775.jpg\",\n",
    "              \"train_7883.jpg\",\n",
    "              \"train_789.jpg\",\n",
    "              \"train_8020.jpg\",\n",
    "              \"train_8146.jpg\",\n",
    "              \"train_882.jpg\",\n",
    "              \"train_903.jpg\",\n",
    "              \"train_924.jpg\"\n",
    "             ),\n",
    "    \"validation\": (\n",
    "              \"val_147.jpg\",\n",
    "              \"val_286.jpg\",\n",
    "              \"val_296.jpg\",\n",
    "              \"val_386.jpg\"\n",
    "             ),\n",
    "    \"test\": (\n",
    "              \"test_132.jpg\",\n",
    "              \"test_1346.jpg\",\n",
    "              \"test_184.jpg\",\n",
    "              \"test_1929.jpg\",\n",
    "              \"test_2028.jpg\",\n",
    "              \"test_22.jpg\",\n",
    "              \"test_2321.jpg\",\n",
    "              \"test_232.jpg\",\n",
    "              \"test_2613.jpg\",\n",
    "              \"test_2643.jpg\",\n",
    "              \"test_274.jpg\",\n",
    "              \"test_2878.jpg\",\n",
    "              \"test_521.jpg\",\n",
    "              \"test_853.jpg\",\n",
    "              \"test_910.jpg\",\n",
    "              \"test_923.jpg\"\n",
    "             ),             \n",
    "}\n",
    "\n",
    "for subset, filenames in CORRUPTED_IMAGES.items():\n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(images_path, filename)\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "            #print(f\"Removed: {file_path}\")\n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"File not found: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- addressing OOM issue: reduce image size(4160x2336 to 416, 233), limit batch size, use tensorflow api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "def load_and_preprocess_data(annotations_file, images_path, subset_size=0.01, img_size=(416, 233), batch_size=256):\n",
    "    df = pd.read_csv(annotations_file)\n",
    "    subset = \"train\" if \"train\" in str(annotations_file) else \"validation\" if \"val\" in str(annotations_file) else \"test\"\n",
    "    corrupted_images = CORRUPTED_IMAGES.get(subset, [])\n",
    "    df = df[~df.iloc[:, 0].isin(corrupted_images)]\n",
    "    df = df.sample(frac=subset_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    images_path = str(images_path)  # Convert images_path to string\n",
    "\n",
    "    def preprocess(image_name, x1, y1, x2, y2, w, h):\n",
    "        image_name = image_name.numpy().decode('utf-8')\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, img_size)  # Resize image\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)  # Normalize to [0, 1]\n",
    "\n",
    "        x1, x2 = x1 / w, x2 / w\n",
    "        y1, y2 = y1 / h, y2 / h\n",
    "\n",
    "        bbox = tf.convert_to_tensor([y1, x1, y2, x2], dtype=tf.float32)  # TensorFlow format: [ymin, xmin, ymax, xmax]\n",
    "        return img, bbox\n",
    "\n",
    "    # Convert dataframe columns to tensors\n",
    "    image_names = tf.convert_to_tensor(df[df.columns[0]].values, dtype=tf.string)\n",
    "    x1 = tf.convert_to_tensor(df[df.columns[1]].values.astype(float), dtype=tf.float32)\n",
    "    y1 = tf.convert_to_tensor(df[df.columns[2]].values.astype(float), dtype=tf.float32)\n",
    "    x2 = tf.convert_to_tensor(df[df.columns[3]].values.astype(float), dtype=tf.float32)\n",
    "    y2 = tf.convert_to_tensor(df[df.columns[4]].values.astype(float), dtype=tf.float32)\n",
    "    w = tf.convert_to_tensor(df[df.columns[6]].values.astype(float), dtype=tf.float32)\n",
    "    h = tf.convert_to_tensor(df[df.columns[7]].values.astype(float), dtype=tf.float32)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_names, x1, y1, x2, y2, w, h))\n",
    "    dataset = dataset.map(lambda img, x1, y1, x2, y2, w, h: tf.py_function(preprocess, [img, x1, y1, x2, y2, w, h], [tf.float32, tf.float32]))\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 02:31:13.743859: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-11-21 02:31:13.744307: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "subset_size = 0.1  # You can adjust this value as needed\n",
    "img_size = (416, 233)  # Resize images to 224x224\n",
    "batch_size = 64  # Number of samples per batch\n",
    "\n",
    "train_data = load_and_preprocess_data(annotations_dir / 'annotations_train.csv', images_path, subset_size, img_size, batch_size)\n",
    "val_data = load_and_preprocess_data(annotations_dir / 'annotations_val.csv', images_path, subset_size, img_size, batch_size)\n",
    "test_data = load_and_preprocess_data(annotations_dir / 'annotations_test.csv', images_path, subset_size, img_size, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SKU_DATASET_DIR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/limjohn/repos/term7cv/VGG.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data_num, \u001b[39mset\u001b[39m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m image_num \u001b[39m=\u001b[39m SKU_DATASET_DIR \u001b[39m+\u001b[39m \u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mset\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mdata_num\u001b[39m}\u001b[39;00m\u001b[39m.jpg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m annotations_dir \u001b[39m=\u001b[39m Path(sku_dataset_dir) \u001b[39m/\u001b[39m \u001b[39m'\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m annotation_set_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(SKU_DATASET_DIR \u001b[39m+\u001b[39m \u001b[39mrf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mannotations\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mannotations_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mset\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SKU_DATASET_DIR' is not defined"
     ]
    }
   ],
   "source": [
    "data_num, set = 0, 'test' \n",
    "image_num = sku_dataset_dir + rf'\\images\\{set}_{data_num}.jpg'\n",
    "annotations_dir = Path(sku_dataset_dir) / 'annotations'\n",
    "annotation_set_df = pd.read_csv(sku_dataset_dir + rf'\\annotations\\annotations_{set}.csv')\n",
    "annotation_num = annotation_set_df[annotation_set_df.iloc[:, 0] == f'{set}_{data_num}.jpg']\n",
    "\n",
    "def display_image_with_annotations(image_path, annotations_df):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # Add the bounding boxes\n",
    "    for index, row in annotations_df.iterrows():\n",
    "        x1, y1, x2, y2 = row[1], row[2], row[3], row[4]\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the image with annotations\n",
    "display_image_with_annotations(image_num, annotation_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG16 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:54:15.078033: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - ETA: 0s - loss: 7.1306 - auc_24: 0.7101 - precision_22: 1.0000 - recall_22: 0.4093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:55:41.621072: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to training_1/cp.ckpt\n",
      "19/19 [==============================] - 99s 5s/step - loss: 7.1306 - auc_24: 0.7101 - precision_22: 1.0000 - recall_22: 0.4093 - val_loss: 7.3528 - val_auc_24: 0.7039 - val_precision_22: 1.0000 - val_recall_22: 0.3994\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze the VGG16 layers\n",
    "\n",
    "checkpoint_path = \"training_1/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "model = models.Sequential(\n",
    "    [\n",
    "        Input(shape=(416, 233, 3)),\n",
    "        preprocessing.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "        base_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(\n",
    "            4, activation=\"sigmoid\"\n",
    "        ),  # 4 values for bounding box (x, y, width, height)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_data,\n",
    "    callbacks=[cp_callback]\n",
    ")\n",
    "model.save(\"my_model_VGG.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:57:48.939331: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 7s 2s/step - loss: 7.3528 - auc_24: 0.7039 - precision_22: 1.0000 - recall_22: 0.3994\n",
      "Loss on validation data: [7.352823734283447, 0.7039105892181396, 1.0, 0.39944133162498474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:57:55.494481: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 5s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:58:04.106330: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 275ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 22:58:06.039677: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 293ms/step\n",
      "Average IoU: 0.3156541883945465\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model_VGG.keras')\n",
    "iou = tf.keras.metrics.MeanIoU(num_classes=2) \n",
    "\n",
    "eval_results = model.evaluate(val_data)\n",
    "print(\"Loss on validation data:\", eval_results)\n",
    "\n",
    "predictions = model.predict(val_data)\n",
    "\n",
    "for images, true_boxes in val_data:\n",
    "    # Get model predictions\n",
    "    pred_boxes = model.predict(images)\n",
    "    # Update IoU metric\n",
    "    iou.update_state(true_boxes, pred_boxes)\n",
    "\n",
    "# Get the final IoU\n",
    "average_iou = iou.result().numpy()\n",
    "print(f'Average IoU: {average_iou}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 02:32:21.769711: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881/1881 [==============================] - ETA: 0s - loss: 7.3793 - auc_1: 0.7801 - precision_1: 0.9992 - recall_1: 0.7068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 04:19:03.197133: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to training_2/cp.ckpt\n",
      "1881/1881 [==============================] - 6875s 4s/step - loss: 7.3793 - auc_1: 0.7801 - precision_1: 0.9992 - recall_1: 0.7068 - val_loss: 7.4381 - val_auc_1: 0.7594 - val_precision_1: 0.9989 - val_recall_1: 0.7513\n",
      "Epoch 2/5\n",
      "1881/1881 [==============================] - ETA: 0s - loss: 7.3994 - auc_1: 0.7858 - precision_1: 0.9992 - recall_1: 0.7237\n",
      "Epoch 2: saving model to training_2/cp.ckpt\n",
      "1881/1881 [==============================] - 6909s 4s/step - loss: 7.3994 - auc_1: 0.7858 - precision_1: 0.9992 - recall_1: 0.7237 - val_loss: 7.4395 - val_auc_1: 0.7596 - val_precision_1: 0.9989 - val_recall_1: 0.7519\n",
      "Epoch 3/5\n",
      "1881/1881 [==============================] - ETA: 0s - loss: 7.4164 - auc_1: 0.7895 - precision_1: 0.9992 - recall_1: 0.7513\n",
      "Epoch 3: saving model to training_2/cp.ckpt\n",
      "1881/1881 [==============================] - 6967s 4s/step - loss: 7.4164 - auc_1: 0.7895 - precision_1: 0.9992 - recall_1: 0.7513 - val_loss: 7.4395 - val_auc_1: 0.7596 - val_precision_1: 0.9989 - val_recall_1: 0.7519\n",
      "Epoch 4/5\n",
      "1881/1881 [==============================] - ETA: 0s - loss: 7.4173 - auc_1: 0.7898 - precision_1: 0.9992 - recall_1: 0.7521\n",
      "Epoch 4: saving model to training_2/cp.ckpt\n",
      "1881/1881 [==============================] - 7021s 4s/step - loss: 7.4173 - auc_1: 0.7898 - precision_1: 0.9992 - recall_1: 0.7521 - val_loss: 7.4395 - val_auc_1: 0.7596 - val_precision_1: 0.9989 - val_recall_1: 0.7519\n",
      "Epoch 5/5\n",
      " 268/1881 [===>..........................] - ETA: 1:34:28 - loss: 7.4256 - auc_1: 0.8016 - precision_1: 0.9993 - recall_1: 0.7522"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/limjohn/repos/term7cv/VGG.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m resnet_model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         Input(shape\u001b[39m=\u001b[39m(\u001b[39m416\u001b[39m, \u001b[39m233\u001b[39m, \u001b[39m3\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m resnet_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     ],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m history \u001b[39m=\u001b[39m resnet_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     train_data, epochs\u001b[39m=\u001b[39;49mEPOCHS, validation_data\u001b[39m=\u001b[39;49mval_data, callbacks\u001b[39m=\u001b[39;49m[cp_callback]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/limjohn/repos/term7cv/VGG.ipynb#X22sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmy_model_RESNET.keras\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/cv-labs-390/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model_2 = ResNet50(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,  # use our own input layer for transfer learning\n",
    "    input_shape=(224, 224, 3),\n",
    ")\n",
    "\n",
    "base_model_2.trainable = False\n",
    "\n",
    "checkpoint_path = \"training_2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, save_weights_only=True, verbose=1\n",
    ")\n",
    "\n",
    "resnet_model = models.Sequential(\n",
    "    [\n",
    "        Input(shape=(416, 233, 3)),\n",
    "        preprocessing.Resizing(224, 224, interpolation=\"bilinear\"),\n",
    "        base_model_2,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation=\"relu\"),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(\n",
    "            4, activation=\"sigmoid\"\n",
    "        ),  # 4 values for bounding box (x, y, width, height)\n",
    "    ]\n",
    ")\n",
    "\n",
    "resnet_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\n",
    "        tf.keras.metrics.AUC(),\n",
    "        tf.keras.metrics.Precision(),\n",
    "        tf.keras.metrics.Recall(),\n",
    "    ],\n",
    ")\n",
    "\n",
    "history = resnet_model.fit(\n",
    "    train_data, epochs=EPOCHS, validation_data=val_data, callbacks=[cp_callback]\n",
    ")\n",
    "model.save(\"my_model_RESNET.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 2s/step - loss: 7.3528 - auc_24: 0.7039 - precision_22: 1.0000 - recall_22: 0.3994\n",
      "Loss on validation data: [7.352823734283447, 0.7039105892181396, 1.0, 0.39944133162498474]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 23:00:22.141202: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 6s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 23:00:31.528740: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 292ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 23:00:33.506038: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 375ms/step\n",
      "Average IoU: 0.3156541883945465\n"
     ]
    }
   ],
   "source": [
    "eval_results = model.evaluate(val_data)\n",
    "print(\"Loss on validation data:\", eval_results)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('my_model_RESNET.keras')\n",
    "iou = tf.keras.metrics.MeanIoU(num_classes=2) \n",
    "\n",
    "predictions = model.predict(val_data)\n",
    "\n",
    "for images, true_boxes in val_data:\n",
    "    # Get model predictions\n",
    "    pred_boxes = model.predict(images)\n",
    "    # Update IoU metric\n",
    "    iou.update_state(true_boxes, pred_boxes)\n",
    "\n",
    "# Get the final IoU\n",
    "average_iou = iou.result().numpy()\n",
    "print(f'Average IoU: {average_iou}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv-labs-390",
   "language": "python",
   "name": "cv-labs-390"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
